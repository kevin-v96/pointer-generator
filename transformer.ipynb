{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b389163a-d1d6-4635-b7a2-33ff05ba3ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-2.7.1-py3-none-any.whl (451 kB)\n",
      "Collecting transformers[sentencepiece]\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (2.27.1)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
      "  Using cached huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Using cached pyarrow-10.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (2022.5.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets) (1.19.5)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.8.2-py3-none-any.whl (10 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.10.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
      "  Using cached sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in /opt/conda/lib/python3.9/site-packages (from transformers[sentencepiece]) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: tokenizers, sentencepiece, xxhash, regex, pyarrow, multidict, frozenlist, filelock, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.5.1\n",
      "    Uninstalling dill-0.3.5.1:\n",
      "      Successfully uninstalled dill-0.3.5.1\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.7.1 dill-0.3.6 filelock-3.8.2 frozenlist-1.3.3 huggingface-hub-0.11.1 multidict-6.0.3 multiprocess-0.70.14 pyarrow-10.0.1 regex-2022.10.31 responses-0.18.0 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.25.1 xxhash-3.1.0 yarl-1.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c29360-d40c-451d-91b4-a02ac6dfb7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/home/jovyan/.cache/huggingface/datasets/ccdv___cnn_dailymail/3.0.0/3.0.0/0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c54706215847158b9c9e5e77c9c4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 287113\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 13368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 11490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "cnn_dataset = load_dataset(\"ccdv/cnn_dailymail\", \"3.0.0\")\n",
    "cnn_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ada04f6-9bc5-4206-9394-34c0ee6d0db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/jovyan/.cache/huggingface/datasets/ccdv___cnn_dailymail/3.0.0/3.0.0/0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f/cache-094b41cff0477abb.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>> Highlights: Brittany Ferrell, nursing student, was arrested with 12 people on Thursday .\n",
      "They were calling on police take responsibility for Michael Brown's death .\n",
      "Ms Ferrell tweeted as she was arrested, piled in a small wagon with 7 others .\n",
      "They were accused of 'noise disruption', put in orange jumpsuits and cuffed .\n",
      "Officers now being investigated, lawyers claim they 'overstretched powers''\n",
      "\n",
      "'>> Article: A protester in Ferguson was arrested during a demonstration on Thursday night - and live-tweeted her entire experience. Brittany Ferrell, a nursing student at the University of Missouri-Saint Louis, was one of 13 people detained by officers in the conflicted Missouri city for 'noise disruption'. The detention has sparked an investigation by the American Civil Liberties Union as lawyers accuse officers of overstretching their powers. Scroll down for video . Arrested: This is Brittany Ferrell, the nursing student and protester who live-tweeted her arrest in Ferguson . Tweeting in handcuffs, she then took her thousands of followers through the uncertain few hours before the nine women and four men eventually learned what they had done wrong and managed to secure their release. She started with the tweet: 'We are all arrested', apparently referring to herself an other members of the advocacy group Millenial Activists United. 'Tweeting while cuffed,' she wrote, adding: 'We were arrested while on the side walk by the way'. According to her Twitter feed, she was piled into a wagon with seven other people and not told where they were going. Enraged, she then reported the reason the group was arrested: 'They said we violated the \"noise\" ordinance\" it's been 50+ days... NOW they effort a noise ordinance. Bull****'. The group started chanting 'no justice, no peace!' in the back of the wagon. 'They think it's a game,' she said. 'Noise disruption': Ms Ferrell was one of 13 people from the advocacy group Millennial Activists United who were arrested on the side walk during a demonstration (pictured) on Thursday following Michael Brown's death . Her followers informed her clergy were en route to the prison to offer support. Arriving at St Ann Police Station, they were dressed in orange jumpsuits, placed in a cell and 'treated like criminals', while officers said the bail would be doubled for anyone who had been arrested before. Ms Ferrell's bail was set at $2,300, she said. More than three hours after the arrest they were informed they would be released before the night was over, but Ms Ferrell said: 'no one can give us clear answers on anything though. aside from the fact that we will be out and that people are working on it now.' She added: 'Again, all 13 of us are still in jail,' tweeting pictures of the four men and eight women she was sat with. Around an hour later, after crowds swarmed around the police station, the group was released. Local media reported protesters lay on the ground around the arrestees' cars to stop them from being towed. Many of the jail bonds were waived, according to the Argus Streaming News account on Twitter, but not for all. Later, Ms Ferrell told her followers: 'They straight criminalized us. made us stand in a cage. asked us if they turned their backs would we cause trouble. we are protestors!' 'The way we've been treated you would think WE are murderers, con artists, robbers. the mentality of the officers that handled us was insane.' Tony Rothert, an attorney for the American Civil Liberties Union office in St. Louis, said several complaints have been lodged about Thursday's arrests and the ACLU is investigating. 'There do appear to be, if not bogus, at least unnecessary arrests,' said Rothert, who had no details about why the protesters were arrested. He also questioned why they'd be jailed on a 24-hour hold. 'Holding people unnecessarily long, arresting them when it's not necessary — that's an indication of overreach,' Rothert said. 'It certainly appears that the purpose is to discourage people from coming to Ferguson to protest.' Released: The nine women and four men celebrated as they were released and uncuffed hours later . Investigation: Lawyers will now launch an investigation into unnecessary arrests made by Ferguson police . Conflict: Last week, the police department was condemned by the Department of Justice for refusing to effect change as it emerged numerous officers were still not wearing identification or covering their badges . Hundreds of people have been arrested during nearly two months of protests in Ferguson, including journalists and clergy members. The incident comes amid volatile outbursts in the city, where citizens and the Department of Justice are accusing police of failing to take action to repair racial divides. In a letter to the Ferguson Police Department (FPD) last Tuesday, the department said its own investigators have spotted officers without name tags and refusing to identify themselves to members of the public. The practise violates FPD rules, the department pointed out, and is illegal in many states. It added: 'The failure to wear name plates conveys a message to community members that, through anonymity, officers may seek to act with impunity'. Just days later the letter was followed by a second on Friday, as it emerged the first one had been ignored. 'It further was reported to us that some officers affirmatively displaying these bracelets had black tape over their name plates. 'The practice of not wearing, or obscuring, name plates violates your own department's policies, which we advised you earlier this week when we requested that you end the practice imrnediately.' The second letter also addressed the news that officers were sporting 'I Am Darren Wilson' bracelets in an apparent show of solidarity to the man that shot Michael Brown.'\n",
      "\n",
      "'>> Highlights: Twitter has added photo filters to its Android and iOS mobile apps .\n",
      "The addition will help Twitter compete against Facebook-owned Instagram .\n",
      "This is the first time the social network has offered image editing tools .'\n",
      "\n",
      "'>> Article: A day after confirming it had lost the ability to display Instagram images, Twitter has rolled out its own library of retro filters for its Android and iPhone apps. The eight filters are the usual suspects we've come to expect from mobile photo apps, including desaturated, black and white and high contrast. There are auto-adjust and cropping options, as well as a helpful grid view that lets you see what each filter will look like at once. \"The latest versions of Twitter for iPhone and Twitter for Android introduce a few new ways to enhance the images you tweet,\" said Twitter senior designer Coleen Baik in a blog post announcing the new features. She emphasized that images are important to Twitter users, and called photos \"one of the most compelling forms of self-expression.\" The new filters were designed especially for Twitter by photo-editing service Aviary, which also handles edits for various partners such as Flickr and Twitpic. What the effects lack in originality, they will no doubt make up for in popularity. Filters are an easy alternative to tinkering with an image in a photo editor, and their retro aesthetic has helped Instagram get more than 150 million users. Instagram also released an app update Monday, giving its iOS app a fresh new look and adding a new black and white filter. There's a new grid overlay, better tilt-shift effects and a bigger shutter button among other tweaks. Twitter is in direct competition for users and ad dollars with Facebook, which owns Instagram. Until this week, if you shared an Instagram photo on Twitter, it would appear in the expanded tweet. But on Sunday, the companies confirmed that feature was no more, the image replaced with a link to the photo on Instagram's site. The addition of in-app filters is great for Twitter users who didn't like the extra step of launching Instagram or another photo editing app to spruce up images. Twitter first added the ability to include images in tweets a year and a half ago. Slight improvements on the individual social networks might not make up for the larger loss of cross-service functionality, but the split between competitors was inevitable. If you have an Android device, the updated Twitter app is available now in the Google Play store. It is coming soon to Apple's App Store.'\n",
      "\n",
      "'>> Highlights: Judge initially suggested that Carine Patry Hoskins played only a minor role .\n",
      "She was paid £218,000 for her work between July 2011 and November 2012 .\n",
      "Hoskins had an affair with Hacked Off lawyer David Sherbourne .\n",
      "Leveson now says she had also played a key role in drawing up lines of .\n",
      "questioning used by the inquiry’s lead counsel Robert Jay .'\n",
      "\n",
      "'>> Article: By . Jason Groves . PUBLISHED: . 19:31 EST, 14 June 2013 . | . UPDATED: . 19:50 EST, 14 June 2013 . A lawyer for the Leveson Inquiry who had an affair with Hugh Grant’s barrister played a significant role in the inquiry, Lord Justice Leveson admitted last night. The judge had initially suggested that Carine Patry Hoskins played only a minor role in his inquiry, carrying out work that was ‘largely mechanical’. But, following revelations that she was paid £218,000 for her work between July 2011 and November 2012, he came under pressure to reveal more about her role. Affair: Celebrities' barrister Mr Sherborne and  Mrs Patry Hoskins, a . member of Lord Justice Leveson’s legal team,  claim their relationship . did not start until after the inquiry . In a letter to the Tory MP Rob Wilson, . the judge said she had also played a key role in drawing up lines of . questioning used by the inquiry’s lead counsel Robert Jay, as well as . questioning witnesses herself, reviewing witness statements and . researching areas of the law. However the letter said nothing about her role in drawing up the so-called Rule 13 letters outlining the inquiry’s criticisms of the Press before the report was published. Lord Justice Leveson insists the conclusions of his report were his alone. But Mr Wilson said the revelations would underline public concern about whether the Leveson Inquiry’s findings could have been compromised by an affair between Miss Patry Hoskins, a married mother of two, and David Sherbourne, a barrister representing Mr Grant and other celebrities campaigning for regulation of the Press. Miss Patry Hoskins, who became known as the ‘woman on the left’ during the televised hearings, was junior counsel in the team led by  Mr Jay. She went on holiday to the Greek island of Santorini with Mr Sherbourne in August last year – while the inquiry was being conducted. Lord Justice Leveson admitted that the barrister had in fact had a significant role in the inquiry . The pair claim they simply discussed the ‘possibility of a future relationship and decided against it’. They say they changed their minds later and became a couple after the inquiry ended in December. Mr Wilson said: ‘We now learn that Carine Patry Hoskins did indeed  provide important legal advice to the Leveson Inquiry, and dealt with  witnesses and made assessments of the evidence on behalf of the inquiry. This must have had some bearing on the inquiry’s work, whether she had a formal input into the drafting of the report or not. ‘It would be completely inappropriate for someone to carry out these important tasks for an impartial public inquiry while developing such close personal relations with one of the parties’ barristers to the point that they went on holiday together to contemplate an affair. Holiday together: The pair went on a break to the island of Santorini (pictured) in Greece . ‘I am very surprised that Lord Justice Leveson has not already taken the many opportunities available to him to make this clear.’ In his latest letter, Lord Leveson again denied that Miss Patry Hoskins influenced his final report. He said she had ‘no input into the conclusions or recommendations in the report itself’. In April last year Miss Patry Hoskins told people at the inquiry she was helping to draft the Rule 13 letters, which set out criticisms of the Press across 120 pages that appeared largely unchanged in the final report later in the year. The letters were sent to newspaper editors in mid-August, when Miss Patry Hoskins claims she was contemplating an affair with Mr Sherbourne. Lord Justice Leveson’s latest letter makes no reference to her work in this area, focusing on her work in the early stages of the inquiry. He has previously said she merely ‘assisted in the largely mechanical exercise of collecting and organising the evidence to support the generic criticisms’ set out in the letters.'\n"
     ]
    }
   ],
   "source": [
    "def show_samples(dataset, num_samples = 3, seed = 42):\n",
    "    sample = dataset[\"train\"].shuffle(seed = seed).select(range(num_samples))\n",
    "    for example in sample:\n",
    "        print(f\"\\n'>> Highlights: {example['highlights']}'\")\n",
    "        print(f\"\\n'>> Article: {example['article']}'\")\n",
    "        \n",
    "show_samples(cnn_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7d2a4cc-15dc-4741-86ca-af93bf172ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"google/mt5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3746f2-c574-4988-b9a3-aea00b1433ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 30\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"article\"],\n",
    "        max_length = max_input_length,\n",
    "        truncation = True,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"highlights\"], max_length = max_target_length, truncation = True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80e5399-21b7-4e24-97eb-aa8b814aac7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/ccdv___cnn_dailymail/3.0.0/3.0.0/0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f/cache-b2308ecedf4c01b6.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7965747e2ab4bcb94dd2dab6a76310a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/ccdv___cnn_dailymail/3.0.0/3.0.0/0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f/cache-1306a86d8e204937.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = cnn_dataset.map(preprocess_function, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85d23d3e-796b-4bc5-858b-bce13498d0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: rouge_score in /opt/conda/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.19.5)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from rouge_score) (0.15.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from rouge_score) (3.7)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.15.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.64.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2022.10.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11a52a5c-76eb-4588-b846-5bd178dc6e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.19.5)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.7.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.11.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.27.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.4.2)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.64.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2022.5.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.15.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e83c21-b8d1-4614-9a81-5263722f440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c90dcd6-7fcc-4f48-81c5-f6fba799d878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6df7e326-73c6-4eb7-a437-c4f895c4eef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea29a313-f5b0-4047-99c0-6e7e0ad6a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d4b70e2-4a75-4590-82f4-62926775d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "962ce454-1811-4be1-b835-1c0aada8124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_baseline(dataset, metric):\n",
    "    summaries = [three_sentence_summary(text) for text in dataset[\"article\"]]\n",
    "    return metric.compute(predictions = summaries, references = dataset[\"highlights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cc0a227-8fdb-4b1f-b2be-640fe198f4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 39.49, 'rouge2': 17.56, 'rougeL': 24.96, 'rougeLsum': 36.12}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "score = evaluate_baseline(cnn_dataset[\"validation\"], rouge_score)\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_dict = dict((rn, round(score[rn] * 100, 2)) for rn in rouge_names)\n",
    "rouge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f58a08da-f99d-422d-bcaa-cb49fd95436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6304ba6c-a804-4b20-ae92-cc6cc4e3b04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.9/site-packages (0.11.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.9/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface_hub) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface_hub) (3.8.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from huggingface_hub) (4.64.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from huggingface_hub) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface_hub) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface_hub) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface_hub) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10c2f7b0-7045-4fff-86e6-36d1ce159ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f94ea1edba2412e9df8430cd34e5026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5080cce0-748f-4df9-a294-2faffc62d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "num_train_epochs = 8\n",
    "\n",
    "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c328318-4e1c-4b98-937f-061b269dfc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = f\"{model_name}-finetuned-cnn-dailymail\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    weight_decay = 0.01,\n",
    "    save_total_limit = 2,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    predict_with_generate = True,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub = True,\n",
    "    resume_from_checkpoint=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35ef5b37-82d7-41dc-9a48-638c4d7b3c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = rouge_score.compute(predictions = decoded_preds, references = decoded_labels, use_stemmer = True)\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cdbb4fa-d23d-480d-b020-4312459bac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f64d8f2-a94d-4341-acfc-bb49b3aab537",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    cnn_dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81dd17aa-31c2-466b-b5f5-1759d522b34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  1385,    277,    263,  ...,   1866,    282,      1],\n",
       "        [   274,  57063,    271,  ..., 111875,    259,      1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 55984,    321,  21421,    267,  20820,    259, 120514,    345,    288,\n",
       "            287,   2672,    304,    287,  39114,    261,    313,   6975,    272,\n",
       "            277,    270,   3076,   2606,    288,   1689,   5123,    311,  20820,\n",
       "           6559,    263,      1],\n",
       "        [ 26764,    348,  74072,  73533,    259,  18211,  17088,    304,   4836,\n",
       "            259, 147758,    259,    260,    259, 111235,    263,    259,  96037,\n",
       "            288,    419,    329,   2251,    282,   5686,    276,    259,  98158,\n",
       "            259,    260,      1]]), 'decoder_input_ids': tensor([[     0,  55984,    321,  21421,    267,  20820,    259, 120514,    345,\n",
       "            288,    287,   2672,    304,    287,  39114,    261,    313,   6975,\n",
       "            272,    277,    270,   3076,   2606,    288,   1689,   5123,    311,\n",
       "          20820,   6559,    263],\n",
       "        [     0,  26764,    348,  74072,  73533,    259,  18211,  17088,    304,\n",
       "           4836,    259, 147758,    259,    260,    259, 111235,    263,    259,\n",
       "          96037,    288,    419,    329,   2251,    282,   5686,    276,    259,\n",
       "          98158,    259,    260]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
    "data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0818af18-9936-448a-95fc-cab1a248a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d8d4b9b-9447-410f-ba40-367f931b963b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/Summarisation/mt5-small-finetuned-cnn-dailymail is already a clone of https://huggingface.co/MadMarx37/mt5-small-finetuned-cnn-dailymail. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "WARNING:huggingface_hub.repository:/home/jovyan/Summarisation/mt5-small-finetuned-cnn-dailymail is already a clone of https://huggingface.co/MadMarx37/mt5-small-finetuned-cnn-dailymail. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset = tokenized_datasets[\"train\"],\n",
    "    eval_dataset = tokenized_datasets[\"validation\"],\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f58c9e58-ed56-4fd4-9ec0-b422c403752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from ./mt5-small-finetuned-cnn-dailymail/checkpoint-27000.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 287113\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 71784\n",
      "  Number of trainable parameters = 300176768\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 3\n",
      "  Continuing training from global step 27000\n",
      "  Will skip the first 3 epochs then the first 81 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c50153be32d4c7da908667ac037aea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71784' max='71784' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71784/71784 10:39:52, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.161500</td>\n",
       "      <td>1.763996</td>\n",
       "      <td>32.678800</td>\n",
       "      <td>16.940000</td>\n",
       "      <td>28.994000</td>\n",
       "      <td>30.688300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.161500</td>\n",
       "      <td>1.745028</td>\n",
       "      <td>32.812900</td>\n",
       "      <td>17.048000</td>\n",
       "      <td>29.078800</td>\n",
       "      <td>30.810600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.161500</td>\n",
       "      <td>1.737945</td>\n",
       "      <td>32.707400</td>\n",
       "      <td>16.964100</td>\n",
       "      <td>28.974500</td>\n",
       "      <td>30.704300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.161500</td>\n",
       "      <td>1.731707</td>\n",
       "      <td>32.769200</td>\n",
       "      <td>17.011600</td>\n",
       "      <td>29.039500</td>\n",
       "      <td>30.768500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.088600</td>\n",
       "      <td>1.729406</td>\n",
       "      <td>32.835200</td>\n",
       "      <td>17.063300</td>\n",
       "      <td>29.088800</td>\n",
       "      <td>30.822600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-27500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-27500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-27500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-27500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-26000] due to args.save_total_limit\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-26500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-28000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-28000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-28000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-28000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-27000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-28500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-28500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-28500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-28500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-27500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-29000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-29000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-29000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-29000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-28000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-29500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-29500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-29500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-29500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-28500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-30000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-30000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-30000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-30000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-29000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-30500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-30500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-30500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-30500/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-29500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-31000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-31000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-31000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-31000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-30000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-31500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-31500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-31500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-31500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-30500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-32000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-32000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-32000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-32000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-31000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-32500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-32500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-32500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-32500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-31500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-33000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-33000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-33000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-33000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-32000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-33500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-33500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-33500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-33500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-32500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-34000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-34000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-34000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-34000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-33000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-34500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-34500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-34500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-34500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-33500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-35000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-35000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-35000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-35000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-34000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-35500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-35500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-35500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-35500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-34500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 13368\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-36000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-36000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-36000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-36000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-35000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-36500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-36500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-36500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-36500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-35500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-37000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-37000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-37000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-37000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-36000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-37500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-37500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-37500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-37500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-36500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-38000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-38000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-38000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-38000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-37000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-38500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-38500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-38500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-38500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-38500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-38500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-37500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-39000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-39000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-39000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-39000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-38000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-39500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-39500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-39500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-39500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-39500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-39500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-38500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-40000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-40000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-40000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-40000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-40000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-39000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-40500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-40500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-40500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-40500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-40500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-40500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-39500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-41000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-41000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-41000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-41000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-41000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-41000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-40000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-41500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-41500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-41500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-41500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-41500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-41500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-40500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-42000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-42000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-42000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-42000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-42000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-42000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-41000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-42500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-42500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-42500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-42500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-42500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-42500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-41500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-43000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-43000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-43000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-43000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-43000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-43000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-42000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-43500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-43500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-43500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-43500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-43500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-43500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-42500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-44000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-44000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-44000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-44000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-44000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-44000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-43000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-44500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-44500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-44500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-44500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-44500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-44500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-43500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 13368\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-45000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-45000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-45000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-45000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-45000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-45000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-44000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-45500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-45500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-45500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-45500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-45500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-45500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-44500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-46000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-46000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-46000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-46000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-46000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-46000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-45000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-46500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-46500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-46500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-46500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-46500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-46500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-45500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-47000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-47000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-47000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-47000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-47000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-47000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-46000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-47500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-47500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-47500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-47500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-47500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-47500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-46500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-48000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-48000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-48000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-48000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-48000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-48000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-47000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-48500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-48500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-48500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-48500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-48500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-48500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-47500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-49000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-49000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-49000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-49000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-49000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-49000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-48000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-49500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-49500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-49500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-49500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-49500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-49500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-48500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-50000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-50000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-50000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-50000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-50000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-49000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-50500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-50500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-50500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-50500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-50500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-50500/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-49500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-51000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-51000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-51000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-51000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-51000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-51000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-50000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-51500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-51500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-51500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-51500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-51500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-51500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-50500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-52000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-52000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-52000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-52000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-52000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-52000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-51000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-52500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-52500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-52500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-52500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-52500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-52500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-51500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-53000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-53000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-53000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-53000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-53000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-53000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-52000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-53500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-53500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-53500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-53500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-53500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-53500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-52500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 13368\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-54000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-54000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-54000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-54000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-54000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-54000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-53000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-54500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-54500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-54500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-54500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-54500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-54500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-53500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-55000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-55000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-55000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-55000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-55000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-55000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-54000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-55500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-55500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-55500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-55500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-55500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-55500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-54500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-56000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-56000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-56000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-56000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-56000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-56000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-55000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-56500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-56500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-56500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-56500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-56500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-56500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-55500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-57000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-57000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-57000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-57000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-57000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-57000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-56000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-57500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-57500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-57500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-57500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-57500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-57500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-56500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-58000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-58000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-58000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-58000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-58000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-58000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-57000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-58500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-58500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-58500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-58500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-58500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-58500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-57500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-59000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-59000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-59000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-59000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-59000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-59000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-58000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-59500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-59500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-59500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-59500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-59500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-59500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-58500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-60000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-60000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-60000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-60000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-60000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-60000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-59000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-60500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-60500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-60500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-60500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-60500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-60500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-59500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-61000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-61000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-61000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-61000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-61000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-61000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-60000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-61500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-61500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-61500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-61500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-61500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-61500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-60500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-62000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-62000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-62000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-62000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-62000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-62000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-61000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-62500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-62500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-62500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-62500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-62500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-62500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-61500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 13368\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-63000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-63000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-63000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-63000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-63000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-63000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-62000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-63500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-63500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-63500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-63500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-63500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-63500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-62500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-64000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-64000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-64000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-64000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-64000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-64000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-63000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-64500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-64500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-64500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-64500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-64500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-64500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-63500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-65000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-65000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-65000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-65000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-65000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-65000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-64000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-65500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-65500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-65500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-65500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-65500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-65500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-64500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-66000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-66000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-66000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-66000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-66000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-66000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-65000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-66500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-66500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-66500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-66500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-66500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-66500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-65500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-67000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-67000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-67000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-67000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-67000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-67000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-66000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-67500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-67500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-67500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-67500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-67500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-67500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-66500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-68000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-68000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-68000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-68000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-68000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-68000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-67000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-68500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-68500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-68500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-68500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-68500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-68500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-67500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-69000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-69000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-69000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-69000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-69000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-69000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-68000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-69500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-69500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-69500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-69500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-69500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-69500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-68500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-70000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-70000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-70000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-70000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-70000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-70000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-69000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-70500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-70500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-70500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-70500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-70500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-70500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-69500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-71000\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-71000/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-71000/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-71000/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-71000/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-71000/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-70000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail/checkpoint-71500\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/checkpoint-71500/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/checkpoint-71500/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-71500/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/checkpoint-71500/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/checkpoint-71500/spiece.model\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n",
      "Deleting older checkpoint [mt5-small-finetuned-cnn-dailymail/checkpoint-70500] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 13368\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=71784, training_loss=1.3120681161294876, metrics={'train_runtime': 38405.036, 'train_samples_per_second': 59.807, 'train_steps_per_second': 1.869, 'total_flos': 1.2144872799810355e+18, 'train_loss': 1.3120681161294876, 'epoch': 8.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(\"./mt5-small-finetuned-cnn-dailymail/checkpoint-27000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ac63681-19d7-409f-9e4c-26510576b197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 13368\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='418' max='418' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [418/418 03:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.7294056415557861,\n",
       " 'eval_rouge1': 32.8352,\n",
       " 'eval_rouge2': 17.0633,\n",
       " 'eval_rougeL': 29.0888,\n",
       " 'eval_rougeLsum': 30.8226,\n",
       " 'eval_runtime': 222.5164,\n",
       " 'eval_samples_per_second': 60.076,\n",
       " 'eval_steps_per_second': 1.879,\n",
       " 'epoch': 8.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90a22f8f-a28c-4644-930a-0fdc41e67100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to mt5-small-finetuned-cnn-dailymail\n",
      "Configuration saved in mt5-small-finetuned-cnn-dailymail/config.json\n",
      "Model weights saved in mt5-small-finetuned-cnn-dailymail/pytorch_model.bin\n",
      "tokenizer config file saved in mt5-small-finetuned-cnn-dailymail/tokenizer_config.json\n",
      "Special tokens file saved in mt5-small-finetuned-cnn-dailymail/special_tokens_map.json\n",
      "Copy vocab file to mt5-small-finetuned-cnn-dailymail/spiece.model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c913e8f37454470bb398d843fcd8f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e124b4a759446586fe8861d7391a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Dec03_02-12-52_jupyter-mkac283/events.out.tfevents.1670084941.jupyter-mkac283.50273.4: 100%|#…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588a54353f444cda96fa120e5e8ea83d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Dec03_02-12-52_jupyter-mkac283/events.out.tfevents.1670033739.jupyter-mkac283.50273.2: 100%|#…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/MadMarx37/mt5-small-finetuned-cnn-dailymail\n",
      "   f5ab95e..6c8755d  main -> main\n",
      "\n",
      "WARNING:huggingface_hub.repository:remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/MadMarx37/mt5-small-finetuned-cnn-dailymail\n",
      "   f5ab95e..6c8755d  main -> main\n",
      "\n",
      "To https://huggingface.co/MadMarx37/mt5-small-finetuned-cnn-dailymail\n",
      "   6c8755d..dac9127  main -> main\n",
      "\n",
      "WARNING:huggingface_hub.repository:To https://huggingface.co/MadMarx37/mt5-small-finetuned-cnn-dailymail\n",
      "   6c8755d..dac9127  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/MadMarx37/mt5-small-finetuned-cnn-dailymail/commit/6c8755d290d2131ff02d40404ec6c263b3042eec'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712cf3d0-00d8-44d4-b37c-ed571fed5695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
